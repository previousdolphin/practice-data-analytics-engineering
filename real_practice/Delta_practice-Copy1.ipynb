{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9408de70-7715-4c06-93e6-af54d816eb42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b489371-83a5-47b7-b954-c917173ded30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/Users/jesses_fables/Desktop/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/jesses_fables/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /Users/jesses_fables/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6e8bc227-8d38-43f6-9b6b-467668749f87;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 498ms :: artifacts dl 40ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6e8bc227-8d38-43f6-9b6b-467668749f87\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/16ms)\n",
      "25/09/07 11:43:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/07 11:43:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✨ Spark and Delta Lake are ready to go! ✨\n"
     ]
    }
   ],
   "source": [
    "# ==> THIS IS THE MOST IMPORTANT CONFIG <==\n",
    "# Configure Spark Session to use Delta Lake\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"AwesomeDeltaLake\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "# Create the SparkSession\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print(\"✨ Spark and Delta Lake are ready to go! ✨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dec4e501-5d96-4fda-9419-de826233d735",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to Delta table at: fire/tmp/delta-table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/07 11:44:07 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Write complete!\n"
     ]
    }
   ],
   "source": [
    "# Define the path for our Delta table\n",
    "delta_table_path = \"fire/tmp/delta-table\"\n",
    "\n",
    "# Sample data for our table\n",
    "data = spark.createDataFrame([\n",
    "    (1, \"Alice\", 25),\n",
    "    (2, \"Bob\", 30),\n",
    "    (3, \"Charlie\", 35)\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Write the DataFrame to a Delta table\n",
    "print(f\"Writing data to Delta table at: {delta_table_path}\")\n",
    "data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "print(\"✅ Write complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "297405af-3f06-4eb7-95cf-6f9e279dbccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from Delta table:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  3|Charlie| 35|\n",
      "|  1|  Alice| 25|\n",
      "|  2|    Bob| 30|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see what we created!\n",
    "# Read the data back and show it\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "print(\"Reading data from Delta table:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a069f9c9-5062-42af-a3f6-5dcd28bc2ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from Delta table:\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  3|Charlie| 35|\n",
      "|  1|  Alice| 25|\n",
      "|  2|    Bob| 30|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see what we created!\n",
    "# Read the data back and show it\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "print(\"Reading data from Delta table:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b928bc95-f8ef-4865-86bf-5bfb6badcb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data to merge:\n",
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  1|Alicia| 26|\n",
      "|  4| David| 40|\n",
      "+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new DataFrame with new and updated data\n",
    "newData = spark.createDataFrame([\n",
    "    (1, \"Alicia\", 26),        # Update Alice's name and age\n",
    "    (4, \"David\", 40)         # New person\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "print(\"New data to merge:\")\n",
    "newData.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82d19a3e-42d6-4264-860f-6b1ef7ae024b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing merge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merge complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/07 11:44:29 WARN MapPartitionsRDD: RDD 57 was locally checkpointed, its lineage has been truncated and cannot be recomputed after unpersisting\n"
     ]
    }
   ],
   "source": [
    "# Load the Delta table as a DeltaTable object\n",
    "from delta.tables import *\n",
    "deltaTable = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "# Perform the merge operation\n",
    "print(\"Performing merge...\")\n",
    "deltaTable.alias(\"oldData\") \\\n",
    "  .merge(\n",
    "    newData.alias(\"newData\"),\n",
    "    \"oldData.id = newData.id\"\n",
    "  ) \\\n",
    "  .whenMatchedUpdate(set = { \"name\": col(\"newData.name\"), \"age\": col(\"newData.age\") }) \\\n",
    "  .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\"), \"name\": col(\"newData.name\"), \"age\": col(\"newData.age\") }) \\\n",
    "  .execute()\n",
    "\n",
    "print(\"✅ Merge complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eee7bf83-4d5e-432b-85b6-233234aed442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table content after merge:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1| Alicia| 26|\n",
      "|  2|    Bob| 30|\n",
      "|  3|Charlie| 35|\n",
      "|  4|  David| 40|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See the results! Bob and Charlie are untouched.\n",
    "# Alice is updated to Alicia. David is added.\n",
    "print(\"Table content after merge:\")\n",
    "spark.read.format(\"delta\").load(delta_table_path).orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f54ef43-166e-4579-aff8-85c2594dfab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading table at version 0 (before merge):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 25|\n",
      "|  2|    Bob| 30|\n",
      "|  3|Charlie| 35|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's read the *first* version of our table (before the merge)\n",
    "# Version 0 was the initial write.\n",
    "print(\"Reading table at version 0 (before merge):\")\n",
    "df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
    "df_v0.orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2de5a7b-0d08-4f6a-8acd-75175f1b3566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading table at version 1 (after merge):\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1| Alicia| 26|\n",
      "|  2|    Bob| 30|\n",
      "|  3|Charlie| 35|\n",
      "|  4|  David| 40|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's read the *latest* version (after the merge)\n",
    "# Version 1 was the merge operation.\n",
    "print(\"Reading table at version 1 (after merge):\")\n",
    "df_v1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(delta_table_path)\n",
    "df_v1.orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "470e43b5-13c1-4ff4-9c5d-025f75315974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to write data with a different schema...\n",
      "🚨 ERROR! As expected, the write failed.\n",
      "[_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: bcf4e37d-3820-4e1a-b849-58198574cb2a).\n",
      "To enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n",
      "'.option(\"mergeSchema\", \"true\")'.\n",
      "For other operations, set the session configuration\n",
      "spark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\n",
      "specific to the operation for details.\n",
      "\n",
      "Table schema:\n",
      "root\n",
      "-- id: long (nullable = true)\n",
      "-- name: string (nullable = true)\n",
      "-- age: long (nullable = true)\n",
      "\n",
      "\n",
      "Data schema:\n",
      "root\n",
      "-- id: long (nullable = true)\n",
      "-- name: string (nullable = true)\n",
      "-- age: long (nullable = true)\n",
      "-- city: string (nullable = true)\n",
      "\n",
      "         \n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with a different schema (new column 'city')\n",
    "badData = spark.createDataFrame([\n",
    "    (5, \"Eve\", 28, \"Miami\")\n",
    "], [\"id\", \"name\", \"age\", \"city\"])\n",
    "\n",
    "# Let's try to append this...\n",
    "print(\"Attempting to write data with a different schema...\")\n",
    "try:\n",
    "    badData.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n",
    "except Exception as e:\n",
    "    print(\"🚨 ERROR! As expected, the write failed.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bb9437c-4044-4494-9ca5-a5c430342fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing again with schema evolution enabled...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Write successful!\n"
     ]
    }
   ],
   "source": [
    "# Let's retry by enabling schema evolution\n",
    "print(\"Writing again with schema evolution enabled...\")\n",
    "\n",
    "badData.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(delta_table_path)\n",
    "\n",
    "print(\"✅ Write successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c60da7f4-960a-4cf6-8fb1-2cc468c61ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table content after schema evolution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----+\n",
      "| id|   name|age| city|\n",
      "+---+-------+---+-----+\n",
      "|  1| Alicia| 26| NULL|\n",
      "|  2|    Bob| 30| NULL|\n",
      "|  3|Charlie| 35| NULL|\n",
      "|  4|  David| 40| NULL|\n",
      "|  5|    Eve| 28|Miami|\n",
      "+---+-------+---+-----+\n",
      "\n",
      "New schema:\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the new schema and data\n",
    "# The 'city' column has been added for the new row,\n",
    "# and is null for the old rows.\n",
    "print(\"Table content after schema evolution:\")\n",
    "spark.read.format(\"delta\").load(delta_table_path).orderBy(\"id\").show()\n",
    "\n",
    "print(\"New schema:\")\n",
    "spark.read.format(\"delta\").load(delta_table_path).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8ccb2db-080e-4755-92d2-aec76f7ad2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54258467-ccd7-4156-8b7c-ba4f858eb870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/07 11:45:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✨ Spark and Delta Lake are ready to go! ✨\n"
     ]
    }
   ],
   "source": [
    "# ==> THIS IS THE MOST IMPORTANT CONFIG <==\n",
    "# Configure Spark Session to use Delta Lake\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"AwesomeDeltaLake\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "# Create the SparkSession\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print(\"✨ Spark and Delta Lake are ready to go! ✨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96f78bae-7cd9-4cb9-bdc2-c69524465b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----+\n",
      "| id|   name|age| city|\n",
      "+---+-------+---+-----+\n",
      "|  1| Alicia| 26| NULL|\n",
      "|  2|    Bob| 30| NULL|\n",
      "|  3|Charlie| 35| NULL|\n",
      "|  4|  David| 40| NULL|\n",
      "|  5|    Eve| 28|Miami|\n",
      "+---+-------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "df.orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aee843da-a0d9-4d8a-95c5-fa4666f0a8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/07 11:50:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"DataFramePractice\").getOrCreate()\n",
    "\n",
    "# Create our sample data\n",
    "employee_data = [\n",
    "    (1, \"Alice\", \"Engineering\", 120000, \"2021-03-15\", None),\n",
    "    (2, \"Bob\", \"Engineering\", 95000, \"2022-01-20\", 1),\n",
    "    (3, \"Charlie\", \"Sales\", 80000, \"2021-05-10\", 4),\n",
    "    (4, \"David\", \"Sales\", 110000, \"2020-11-01\", None),\n",
    "    (5, \"Eve\", \"HR\", 75000, \"2023-08-30\", 6),\n",
    "    (6, \"Frank\", \"HR\", 92000, \"2019-07-22\", None),\n",
    "    (7, \"Grace\", \"Engineering\", 105000, \"2023-02-12\", 1)\n",
    "]\n",
    "columns = [\"id\", \"name\", \"department\", \"salary\", \"hire_date\", \"manager_id\"]\n",
    "\n",
    "# Create the DataFrame\n",
    "employee_data_df = spark.createDataFrame(employee_data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6d333bd-d50a-4d67-87f0-79f3e367f7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 3) / 3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+------+----------+----------+\n",
      "| id|   name| department|salary| hire_date|manager_id|\n",
      "+---+-------+-----------+------+----------+----------+\n",
      "|  1|  Alice|Engineering|120000|2021-03-15|      NULL|\n",
      "|  2|    Bob|Engineering| 95000|2022-01-20|         1|\n",
      "|  3|Charlie|      Sales| 80000|2021-05-10|         4|\n",
      "|  4|  David|      Sales|110000|2020-11-01|      NULL|\n",
      "|  5|    Eve|         HR| 75000|2023-08-30|         6|\n",
      "|  6|  Frank|         HR| 92000|2019-07-22|      NULL|\n",
      "|  7|  Grace|Engineering|105000|2023-02-12|         1|\n",
      "+---+-------+-----------+------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "employee_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d22baf37-02a9-48d7-9b2c-0e4f5e001261",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "delta_table_path2 = 'files/emp'\n",
    "\n",
    "employee_data_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96065a95-386d-4a5c-a395-94c5d6fbf8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|  Alice|120000|\n",
      "|    Bob| 95000|\n",
      "|Charlie| 80000|\n",
      "|  David|110000|\n",
      "|    Eve| 75000|\n",
      "|  Frank| 92000|\n",
      "|  Grace|105000|\n",
      "+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "employee_data_df.select(\"name\", \"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d3dc2ec-19cc-4742-9cb1-9488129effbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+----------+----------+\n",
      "| id| name| department|salary| hire_date|manager_id|\n",
      "+---+-----+-----------+------+----------+----------+\n",
      "|  7|Grace|Engineering|105000|2023-02-12|         1|\n",
      "|  2|  Bob|Engineering| 95000|2022-01-20|         1|\n",
      "|  1|Alice|Engineering|120000|2021-03-15|      NULL|\n",
      "+---+-----+-----------+------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "employee_data_df.filter(col(\"department\") == \"Engineering\").orderBy(\"id\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f481ad5-4a3e-4192-a1cb-fea2f4888c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee792d78-dfbc-42b6-9b66-f29780987876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "92db5350-c084-4460-bbd2-b1bf36c028db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+----------+----------+\n",
      "| id| name| department|salary| hire_date|manager_id|\n",
      "+---+-----+-----------+------+----------+----------+\n",
      "|  7|Grace|Engineering|105000|2023-02-12|         1|\n",
      "|  2|  Bob|Engineering| 95000|2022-01-20|         1|\n",
      "|  1|Alice|Engineering|120000|2021-03-15|      NULL|\n",
      "+---+-----+-----------+------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "employee_data_df.filter(col(\"department\") == \"Engineering\").orderBy(col(\"id\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554df16b-5075-4138-91f5-b58c49fd9010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e96600-ef55-48d0-aa98-76bef385f5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9b9f990-9041-4227-8494-c027e6e5332a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+----------+----------+\n",
      "| id| name| department|salary| hire_date|manager_id|\n",
      "+---+-----+-----------+------+----------+----------+\n",
      "|  1|Alice|Engineering|120000|2021-03-15|      NULL|\n",
      "|  4|David|      Sales|110000|2020-11-01|      NULL|\n",
      "|  7|Grace|Engineering|105000|2023-02-12|         1|\n",
      "+---+-----+-----------+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_data_df.filter(\"salary > 100000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94d97e81-658c-4870-925f-270b68d81ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+------+----------+----------+-------+\n",
      "| id|   name| department|salary| hire_date|manager_id|  bonus|\n",
      "+---+-------+-----------+------+----------+----------+-------+\n",
      "|  1|  Alice|Engineering|120000|2021-03-15|      NULL|12000.0|\n",
      "|  2|    Bob|Engineering| 95000|2022-01-20|         1| 9500.0|\n",
      "|  3|Charlie|      Sales| 80000|2021-05-10|         4| 8000.0|\n",
      "|  4|  David|      Sales|110000|2020-11-01|      NULL|11000.0|\n",
      "|  5|    Eve|         HR| 75000|2023-08-30|         6| 7500.0|\n",
      "|  6|  Frank|         HR| 92000|2019-07-22|      NULL| 9200.0|\n",
      "|  7|  Grace|Engineering|105000|2023-02-12|         1|10500.0|\n",
      "+---+-------+-----------+------+----------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Let's give everyone a 10% bonus\n",
    "employee_data_df_with_bonus = employee_data_df.withColumn(\"bonus\", col(\"salary\") * 0.10)\n",
    "employee_data_df_with_bonus.show()\n",
    "\n",
    "# We can also use it to extract the year from the hire_date\n",
    "# df.withColumn(\"hire_year\", year(col(\"hire_date\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "619d638f-04e2-4a07-b114-11b11f974aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+-------------+\n",
      "| department|num_employees|avg_salary|total_payroll|\n",
      "+-----------+-------------+----------+-------------+\n",
      "|Engineering|            3| 106666.67|       320000|\n",
      "|      Sales|            2|   95000.0|       190000|\n",
      "|         HR|            2|   83500.0|       167000|\n",
      "+-----------+-------------+----------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year, avg, sum, count, max, round\n",
    "\n",
    "# Let's find the average salary for each department\n",
    "employee_data_df_with_bonus.groupBy(\"department\") \\\n",
    "  .agg(\n",
    "      count(\"*\").alias(\"num_employees\"),\n",
    "      round(avg(\"salary\"), 2).alias(\"avg_salary\"),\n",
    "      sum(\"salary\").alias(\"total_payroll\")\n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f60f711d-44c0-4467-a8fd-85c20967a257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:>                                                         (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+------+----------+----------+-------+\n",
      "| id|   name| department|salary| hire_date|manager_id|  bonus|\n",
      "+---+-------+-----------+------+----------+----------+-------+\n",
      "|  1|  Alice|Engineering|120000|2021-03-15|      NULL|12000.0|\n",
      "|  4|  David|      Sales|110000|2020-11-01|      NULL|11000.0|\n",
      "|  7|  Grace|Engineering|105000|2023-02-12|         1|10500.0|\n",
      "|  2|    Bob|Engineering| 95000|2022-01-20|         1| 9500.0|\n",
      "|  6|  Frank|         HR| 92000|2019-07-22|      NULL| 9200.0|\n",
      "|  3|Charlie|      Sales| 80000|2021-05-10|         4| 8000.0|\n",
      "|  5|    Eve|         HR| 75000|2023-08-30|         6| 7500.0|\n",
      "+---+-------+-----------+------+----------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Sort employees by salary in descending order\n",
    "employee_data_df_with_bonus.orderBy(col(\"salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3e2cc1f7-1aee-4962-b17f-9071d60d14db",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, count, weekofyear, lag, when, lit, desc, rank\n",
    "\n",
    "# --- 1. Setup Spark Session and Create Dummy Data ---\n",
    "spark = SparkSession.builder.appName(\"RedditSurgeAnalysis\").getOrCreate()\n",
    "\n",
    "posts_data = [(\"p1\", \"sub1\", \"2025-08-20 10:00:00\"), (\"p2\", \"sub1\", \"2025-08-28 11:00:00\"),\n",
    "              (\"p3\", \"sub2\", \"2025-08-21 12:00:00\"), (\"p4\", \"sub2\", \"2025-08-29 13:00:00\"),\n",
    "              (\"p5\", \"sub3\", \"2025-08-22 14:00:00\"), (\"p6\", \"sub3\", \"2025-08-30 15:00:00\")]\n",
    "posts = spark.createDataFrame(posts_data, [\"post_id\", \"subreddit_id\", \"created_utc\"]) \\\n",
    "    .withColumn(\"created_utc\", col(\"created_utc\").cast(\"timestamp\"))\n",
    "\n",
    "comments_data = [(\"c1\", \"p1\", \"2025-08-30 15:01:00\"), (\"c2\", \"p1\", \"2025-08-30 15:02:00\"), (\"c3\", \"p2\", \"2025-08-30 15:03:00\"), # sub1\n",
    "                 (\"c4\", \"p3\", \"2025-08-30 15:04:00\"), (\"c5\", \"p4\", \"2025-08-30 15:05:00\"), (\"c6\", \"p4\", \"2025-08-30 15:06:00\"), (\"c7\", \"p4\", \"2025-08-30 15:07:00\"), # sub2\n",
    "                 (\"c8\", \"p5\", \"2025-08-30 15:08:00\"), (\"c9\", \"p6\", \"2025-08-30 15:09:00\"), (\"c10\", \"p6\", \"2025-08-30 15:11:00\")] # sub3\n",
    "comments = spark.createDataFrame(comments_data, [\"comment_id\", \"post_id\", \"created_utc\"]) \\\n",
    "    .withColumn(\"created_utc\", col(\"created_utc\").cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "016cbd59-c0ba-469b-9afd-4e7ae490087c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------------+-------------+\n",
      "|post_id|subreddit_id|        created_utc|comment_count|\n",
      "+-------+------------+-------------------+-------------+\n",
      "|     p1|        sub1|2025-08-20 10:00:00|            2|\n",
      "|     p2|        sub1|2025-08-28 11:00:00|            1|\n",
      "|     p3|        sub2|2025-08-21 12:00:00|            1|\n",
      "|     p4|        sub2|2025-08-29 13:00:00|            3|\n",
      "|     p5|        sub3|2025-08-22 14:00:00|            1|\n",
      "|     p6|        sub3|2025-08-30 15:00:00|            2|\n",
      "+-------+------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments_per_post = comments.groupBy(\"post_id\").agg(count(\"comment_id\").alias(\"comment_count\"))\n",
    "\n",
    "post_comments = posts.join(comments_per_post, \"post_id\")\n",
    "post_comments.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
