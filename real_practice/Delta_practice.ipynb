{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9408de70-7715-4c06-93e6-af54d816eb42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b489371-83a5-47b7-b954-c917173ded30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/Users/jesses_fables/Desktop/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/jesses_fables/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /Users/jesses_fables/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-365d4173-dcbe-47f6-8a33-99bd7dcd8012;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-spark_2.13/4.0.0/delta-spark_2.13-4.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-spark_2.13;4.0.0!delta-spark_2.13.jar (1203ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/4.0.0/delta-storage-4.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;4.0.0!delta-storage.jar (70ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.13.1/antlr4-runtime-4.13.1.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.13.1!antlr4-runtime.jar (49ms)\n",
      ":: resolution report :: resolve 4117ms :: artifacts dl 1331ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-365d4173-dcbe-47f6-8a33-99bd7dcd8012\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (7933kB/95ms)\n",
      "25/09/07 11:35:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/07 11:35:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ¨ Spark and Delta Lake are ready to go! âœ¨\n"
     ]
    }
   ],
   "source": [
    "# ==> THIS IS THE MOST IMPORTANT CONFIG <==\n",
    "# Configure Spark Session to use Delta Lake\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"AwesomeDeltaLake\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "# Create the SparkSession\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print(\"âœ¨ Spark and Delta Lake are ready to go! âœ¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dec4e501-5d96-4fda-9419-de826233d735",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to Delta table at: /tmp/delta-table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/07 11:35:59 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Write complete!\n"
     ]
    }
   ],
   "source": [
    "# Define the path for our Delta table\n",
    "delta_table_path = \"/tmp/delta-table\"\n",
    "\n",
    "# Sample data for our table\n",
    "data = spark.createDataFrame([\n",
    "    (1, \"Alice\", 25),\n",
    "    (2, \"Bob\", 30),\n",
    "    (3, \"Charlie\", 35)\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Write the DataFrame to a Delta table\n",
    "print(f\"Writing data to Delta table at: {delta_table_path}\")\n",
    "data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "print(\"âœ… Write complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "297405af-3f06-4eb7-95cf-6f9e279dbccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from Delta table:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  3|Charlie| 35|\n",
      "|  1|  Alice| 25|\n",
      "|  2|    Bob| 30|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see what we created!\n",
    "# Read the data back and show it\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "print(\"Reading data from Delta table:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a069f9c9-5062-42af-a3f6-5dcd28bc2ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from Delta table:\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  3|Charlie| 35|\n",
      "|  1|  Alice| 25|\n",
      "|  2|    Bob| 30|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see what we created!\n",
    "# Read the data back and show it\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "print(\"Reading data from Delta table:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b928bc95-f8ef-4865-86bf-5bfb6badcb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data to merge:\n",
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  1|Alicia| 26|\n",
      "|  4| David| 40|\n",
      "+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new DataFrame with new and updated data\n",
    "newData = spark.createDataFrame([\n",
    "    (1, \"Alicia\", 26),        # Update Alice's name and age\n",
    "    (4, \"David\", 40)         # New person\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "print(\"New data to merge:\")\n",
    "newData.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82d19a3e-42d6-4264-860f-6b1ef7ae024b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing merge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merge complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/07 11:37:21 WARN MapPartitionsRDD: RDD 57 was locally checkpointed, its lineage has been truncated and cannot be recomputed after unpersisting\n"
     ]
    }
   ],
   "source": [
    "# Load the Delta table as a DeltaTable object\n",
    "from delta.tables import *\n",
    "deltaTable = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "# Perform the merge operation\n",
    "print(\"Performing merge...\")\n",
    "deltaTable.alias(\"oldData\") \\\n",
    "  .merge(\n",
    "    newData.alias(\"newData\"),\n",
    "    \"oldData.id = newData.id\"\n",
    "  ) \\\n",
    "  .whenMatchedUpdate(set = { \"name\": col(\"newData.name\"), \"age\": col(\"newData.age\") }) \\\n",
    "  .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\"), \"name\": col(\"newData.name\"), \"age\": col(\"newData.age\") }) \\\n",
    "  .execute()\n",
    "\n",
    "print(\"âœ… Merge complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eee7bf83-4d5e-432b-85b6-233234aed442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table content after merge:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1| Alicia| 26|\n",
      "|  2|    Bob| 30|\n",
      "|  3|Charlie| 35|\n",
      "|  4|  David| 40|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See the results! Bob and Charlie are untouched.\n",
    "# Alice is updated to Alicia. David is added.\n",
    "print(\"Table content after merge:\")\n",
    "spark.read.format(\"delta\").load(delta_table_path).orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f54ef43-166e-4579-aff8-85c2594dfab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading table at version 0 (before merge):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 25|\n",
      "|  2|    Bob| 30|\n",
      "|  3|Charlie| 35|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's read the *first* version of our table (before the merge)\n",
    "# Version 0 was the initial write.\n",
    "print(\"Reading table at version 0 (before merge):\")\n",
    "df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
    "df_v0.orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2de5a7b-0d08-4f6a-8acd-75175f1b3566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading table at version 1 (after merge):\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1| Alicia| 26|\n",
      "|  2|    Bob| 30|\n",
      "|  3|Charlie| 35|\n",
      "|  4|  David| 40|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's read the *latest* version (after the merge)\n",
    "# Version 1 was the merge operation.\n",
    "print(\"Reading table at version 1 (after merge):\")\n",
    "df_v1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(delta_table_path)\n",
    "df_v1.orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "470e43b5-13c1-4ff4-9c5d-025f75315974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to write data with a different schema...\n",
      "ðŸš¨ ERROR! As expected, the write failed.\n",
      "[_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: e1e7b773-9c0c-40e1-8a4d-9ace3f45cb8e).\n",
      "To enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n",
      "'.option(\"mergeSchema\", \"true\")'.\n",
      "For other operations, set the session configuration\n",
      "spark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\n",
      "specific to the operation for details.\n",
      "\n",
      "Table schema:\n",
      "root\n",
      "-- id: long (nullable = true)\n",
      "-- name: string (nullable = true)\n",
      "-- age: long (nullable = true)\n",
      "\n",
      "\n",
      "Data schema:\n",
      "root\n",
      "-- id: long (nullable = true)\n",
      "-- name: string (nullable = true)\n",
      "-- age: long (nullable = true)\n",
      "-- city: string (nullable = true)\n",
      "\n",
      "         \n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with a different schema (new column 'city')\n",
    "badData = spark.createDataFrame([\n",
    "    (5, \"Eve\", 28, \"Miami\")\n",
    "], [\"id\", \"name\", \"age\", \"city\"])\n",
    "\n",
    "# Let's try to append this...\n",
    "print(\"Attempting to write data with a different schema...\")\n",
    "try:\n",
    "    badData.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n",
    "except Exception as e:\n",
    "    print(\"ðŸš¨ ERROR! As expected, the write failed.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bb9437c-4044-4494-9ca5-a5c430342fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing again with schema evolution enabled...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Write successful!\n"
     ]
    }
   ],
   "source": [
    "# Let's retry by enabling schema evolution\n",
    "print(\"Writing again with schema evolution enabled...\")\n",
    "\n",
    "badData.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(delta_table_path)\n",
    "\n",
    "print(\"âœ… Write successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c60da7f4-960a-4cf6-8fb1-2cc468c61ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table content after schema evolution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----+\n",
      "| id|   name|age| city|\n",
      "+---+-------+---+-----+\n",
      "|  1| Alicia| 26| NULL|\n",
      "|  2|    Bob| 30| NULL|\n",
      "|  3|Charlie| 35| NULL|\n",
      "|  4|  David| 40| NULL|\n",
      "|  5|    Eve| 28|Miami|\n",
      "+---+-------+---+-----+\n",
      "\n",
      "New schema:\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the new schema and data\n",
    "# The 'city' column has been added for the new row,\n",
    "# and is null for the old rows.\n",
    "print(\"Table content after schema evolution:\")\n",
    "spark.read.format(\"delta\").load(delta_table_path).orderBy(\"id\").show()\n",
    "\n",
    "print(\"New schema:\")\n",
    "spark.read.format(\"delta\").load(delta_table_path).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8ccb2db-080e-4755-92d2-aec76f7ad2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully removed the Delta table at /tmp/delta-table\n"
     ]
    }
   ],
   "source": [
    "# It's good practice to clean up the files\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(delta_table_path)\n",
    "    print(f\"âœ… Successfully removed the Delta table at {delta_table_path}\")\n",
    "except OSError as e:\n",
    "    print(f\"Error: {e.strerror}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54258467-ccd7-4156-8b7c-ba4f858eb870",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stop the Spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f78bae-7cd9-4cb9-bdc2-c69524465b68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
