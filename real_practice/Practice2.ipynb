{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6210889f-6101-4019-8efe-14a46a394c3a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### The Problem: Log File Correlator\n",
    "\n",
    "You are a backend engineer for a large web service. The service generates two distinct log files:\n",
    "\n",
    "1.  **`requests.log`**: Records when a request is received.\n",
    "      * Format: `TIMESTAMP_ISO8601,REQUEST_ID,USER_ID`\n",
    "      * Example: `2025-09-07T12:15:01.123Z,req-abc,user-123`\n",
    "2.  **`responses.log`**: Records when a response is sent.\n",
    "      * Format: `TIMESTAMP_ISO8601,REQUEST_ID,STATUS_CODE`\n",
    "      * Example: `2025-09-07T12:15:01.345Z,req-abc,200`\n",
    "\n",
    "Your task is to write a Python script that **correlates** these two logs and produces a single, combined JSON output. For each request, the output should include the `request_id`, `user_id`, `status_code`, and a calculated `duration_ms`.\n",
    "\n",
    "#### The Challenges (What makes it \"hard\")\n",
    "\n",
    "  * **Scalability**: The log files are **too large to fit into memory**.\n",
    "  * **Unordered Entries**: The logs are not guaranteed to be in chronological order. A response might be logged before its corresponding request.\n",
    "  * **Orphaned Entries**: A `request_id` might appear in one file but not the other due to network errors or crashes.\n",
    "\n",
    "-----\n",
    "\n",
    "### Level 1: Junior Engineer Solution (\"It Works\")\n",
    "\n",
    "This solution correctly solves the problem for small files but ignores the scalability and robustness constraints. It's a good starting point that demonstrates basic Python skills.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "  * Reads entire files into memory.\n",
    "  * Uses multiple loops and basic dictionaries.\n",
    "  * Minimal error handling.\n",
    "  * Contained within a single script or function.\n",
    "\n",
    "<!-- end list -->\n",
    "\n",
    "```python\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def junior_correlator(requests_file, responses_file, output_file):\n",
    "    requests_data = {}\n",
    "    # 1. Read all requests into a dictionary\n",
    "    with open(requests_file, 'r') as f:\n",
    "        for line in f:\n",
    "            timestamp_str, request_id, user_id = line.strip().split(',')\n",
    "            requests_data[request_id] = {\n",
    "                'start_time': datetime.fromisoformat(timestamp_str.replace('Z', '+00:00')),\n",
    "                'user_id': user_id\n",
    "            }\n",
    "\n",
    "    responses_data = {}\n",
    "    # 2. Read all responses into another dictionary\n",
    "    with open(responses_file, 'r') as f:\n",
    "        for line in f:\n",
    "            timestamp_str, request_id, status_code = line.strip().split(',')\n",
    "            responses_data[request_id] = {\n",
    "                'end_time': datetime.fromisoformat(timestamp_str.replace('Z', '+00:00')),\n",
    "                'status_code': int(status_code)\n",
    "            }\n",
    "\n",
    "    results = []\n",
    "    # 3. Loop through requests to find matching responses\n",
    "    for req_id, req_info in requests_data.items():\n",
    "        if req_id in responses_data:\n",
    "            resp_info = responses_data[req_id]\n",
    "            duration = (resp_info['end_time'] - req_info['start_time']).total_seconds() * 1000\n",
    "            \n",
    "            results.append({\n",
    "                'request_id': req_id,\n",
    "                'user_id': req_info['user_id'],\n",
    "                'status_code': resp_info['status_code'],\n",
    "                'duration_ms': int(duration)\n",
    "            })\n",
    "\n",
    "    # 4. Write all results at once\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "# Example Usage (assuming you create these files):\n",
    "# junior_correlator('requests.log', 'responses.log', 'output_junior.json')\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "### Level 2: Mid-Level Engineer Solution (\"It's Well-Crafted\")\n",
    "\n",
    "This solution addresses some of the junior version's shortcomings. It uses better data structures and practices, showing an understanding of code organization and efficiency, though it still has memory limitations.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "  * Uses a single pass over the second file to enrich data from the first.\n",
    "  * Handles potential errors during parsing (e.g., a malformed line).\n",
    "  * Code is broken down into logical functions.\n",
    "  * Uses type hints and generators for better memory management of the *output*.\n",
    "\n",
    "<!-- end list -->\n",
    "\n",
    "```python\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Iterator\n",
    "\n",
    "def parse_log_line(line: str) -> Dict[str, Any]:\n",
    "    \"\"\"Parses a single log line and handles potential errors.\"\"\"\n",
    "    try:\n",
    "        parts = line.strip().split(',')\n",
    "        timestamp = datetime.fromisoformat(parts[0].replace('Z', '+00:00'))\n",
    "        return {'timestamp': timestamp, 'parts': parts[1:]}\n",
    "    except (ValueError, IndexError):\n",
    "        # Log this error in a real system\n",
    "        return None\n",
    "\n",
    "def mid_level_correlator(requests_file: str, responses_file: str) -> Iterator[Dict[str, Any]]:\n",
    "    \"\"\"Correlates logs and yields results one by one.\"\"\"\n",
    "    pending_requests = {}\n",
    "    \n",
    "    # Pass 1: Process all requests first\n",
    "    with open(requests_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parsed = parse_log_line(line)\n",
    "            if parsed:\n",
    "                req_id, user_id = parsed['parts']\n",
    "                pending_requests[req_id] = {\n",
    "                    'start_time': parsed['timestamp'],\n",
    "                    'user_id': user_id\n",
    "                }\n",
    "\n",
    "    # Pass 2: Process responses and find matches\n",
    "    with open(responses_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parsed = parse_log_line(line)\n",
    "            if not parsed:\n",
    "                continue\n",
    "            \n",
    "            resp_id, status_code = parsed['parts']\n",
    "            if resp_id in pending_requests:\n",
    "                req_info = pending_requests[resp_id]\n",
    "                duration = (parsed['timestamp'] - req_info['start_time']).total_seconds() * 1000\n",
    "                \n",
    "                yield {\n",
    "                    'request_id': resp_id,\n",
    "                    'user_id': req_info['user_id'],\n",
    "                    'status_code': int(status_code),\n",
    "                    'duration_ms': int(duration)\n",
    "                }\n",
    "                # Remove to handle potential duplicate request_ids\n",
    "                del pending_requests[resp_id]\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the correlation and write to a file.\"\"\"\n",
    "    with open('output_mid.json', 'w') as f:\n",
    "        # The generator approach prevents building a huge list in memory\n",
    "        results = list(mid_level_correlator('requests.log', 'responses.log'))\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "### Level 3: Senior Engineer Solution (\"It's Production-Ready\")\n",
    "\n",
    "This solution is designed for scalability and robustness. It correctly handles the \"too large for memory\" constraint by processing the files in a streaming fashion.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "  * **Streaming approach**: Never holds the full dataset in memory. Memory is only used for *in-flight* requests.\n",
    "  * **Object-Oriented Design**: Encapsulates logic in a class, making it maintainable and testable.\n",
    "  * **Robust Error Handling**: Uses Python's `logging` module to report issues like orphaned entries.\n",
    "  * **Configurability**: Uses `argparse` to accept file paths from the command line, making it a flexible tool.\n",
    "\n",
    "<!-- end list -->\n",
    "\n",
    "```python\n",
    "import json\n",
    "import logging\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Iterator\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class LogCorrelator:\n",
    "    \"\"\"\n",
    "    Correlates request and response logs in a memory-efficient, streaming manner.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.pending_requests: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    def _process_request_line(self, line: str):\n",
    "        try:\n",
    "            timestamp_str, req_id, user_id = line.strip().split(',')\n",
    "            # If a response arrived first, complete the record\n",
    "            if req_id in self.pending_requests and 'end_time' in self.pending_requests[req_id]:\n",
    "                self.pending_requests[req_id].update({\n",
    "                    'start_time': datetime.fromisoformat(timestamp_str.replace('Z', '+00:00')),\n",
    "                    'user_id': user_id\n",
    "                })\n",
    "            else: # Otherwise, store the request data\n",
    "                self.pending_requests[req_id] = {\n",
    "                    'start_time': datetime.fromisoformat(timestamp_str.replace('Z', '+00:00')),\n",
    "                    'user_id': user_id\n",
    "                }\n",
    "        except (ValueError, IndexError):\n",
    "            logging.warning(f\"Skipping malformed request line: {line.strip()}\")\n",
    "\n",
    "    def _process_response_line(self, line: str) -> Iterator[Dict[str, Any]]:\n",
    "        try:\n",
    "            timestamp_str, resp_id, status_code = line.strip().split(',')\n",
    "            end_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))\n",
    "\n",
    "            # If the request arrived first, complete and yield the record\n",
    "            if resp_id in self.pending_requests and 'start_time' in self.pending_requests[resp_id]:\n",
    "                req_info = self.pending_requests.pop(resp_id) # Pop to free memory\n",
    "                duration = (end_time - req_info['start_time']).total_seconds() * 1000\n",
    "                yield {\n",
    "                    'request_id': resp_id,\n",
    "                    'user_id': req_info['user_id'],\n",
    "                    'status_code': int(status_code),\n",
    "                    'duration_ms': int(duration)\n",
    "                }\n",
    "            else: # Otherwise, store the response as pending\n",
    "                self.pending_requests[resp_id] = {\n",
    "                    'end_time': end_time,\n",
    "                    'status_code': int(status_code)\n",
    "                }\n",
    "        except (ValueError, IndexError):\n",
    "            logging.warning(f\"Skipping malformed response line: {line.strip()}\")\n",
    "\n",
    "    def correlate(self, requests_file: str, responses_file: str) -> Iterator[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Processes both log files and yields correlated results.\n",
    "        This approach assumes files are of similar size and processes them \"together\".\n",
    "        A more advanced solution for disparate file sizes might process one fully first.\n",
    "        \"\"\"\n",
    "        logging.info(\"Starting log correlation.\")\n",
    "        with open(requests_file, 'r') as req_f, open(responses_file, 'r') as resp_f:\n",
    "            # In a real-world scenario with massive files, you might read them in chunks\n",
    "            # or use a more sophisticated streaming library.\n",
    "            for req_line, resp_line in zip(req_f, resp_f):\n",
    "                self._process_request_line(req_line)\n",
    "                yield from self._process_response_line(resp_line)\n",
    "\n",
    "        # After files are processed, log any remaining orphaned entries\n",
    "        orphaned_count = len(self.pending_requests)\n",
    "        if orphaned_count > 0:\n",
    "            logging.warning(f\"Found {orphaned_count} orphaned log entries.\")\n",
    "        logging.info(\"Correlation complete.\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Correlate web service log files.\")\n",
    "    parser.add_argument(\"requests_file\", help=\"Path to the requests log file.\")\n",
    "    parser.add_argument(\"responses_file\", help=\"Path to the responses log file.\")\n",
    "    parser.add_argument(\"output_file\", help=\"Path for the JSON output file.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    correlator = LogCorrelator()\n",
    "    with open(args.output_file, 'w') as f:\n",
    "        results = list(correlator.correlate(args.requests_file, args.responses_file))\n",
    "        json.dump(results, f, indent=2)\n",
    "    logging.info(f\"Successfully wrote {len(results)} records to {args.output_file}\")\n",
    "\n",
    "# To run from the command line:\n",
    "# python your_script_name.py requests.log responses.log output_senior.json\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15649493-b186-424a-8a34-27f5caec25bf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def junior_correlator(requests_file, responses_file, output_file):\n",
    "    requests_data = {}\n",
    "    # 1. Read all requests into a dictionary\n",
    "    with open(requests_file, 'r') as f:\n",
    "        for line in f:\n",
    "            timestamp_str, request_id, user_id = line.strip().split(',')\n",
    "            requests_data[request_id] = {\n",
    "                'start_time': datetime.fromisoformat(timestamp_str.replace('Z', '+00:00')),\n",
    "                'user_id': user_id\n",
    "            }\n",
    "\n",
    "    responses_data = {}\n",
    "    # 2. Read all responses into another dictionary\n",
    "    with open(responses_file, 'r') as f:\n",
    "        for line in f:\n",
    "            timestamp_str, request_id, status_code = line.strip().split(',')\n",
    "            responses_data[request_id] = {\n",
    "                'end_time': datetime.fromisoformat(timestamp_str.replace('Z', '+00:00')),\n",
    "                'status_code': int(status_code)\n",
    "            }\n",
    "\n",
    "    results = []\n",
    "    # 3. Loop through requests to find matching responses\n",
    "    for req_id, req_info in requests_data.items():\n",
    "        if req_id in responses_data:\n",
    "            resp_info = responses_data[req_id]\n",
    "            duration = (resp_info['end_time'] - req_info['start_time']).total_seconds() * 1000\n",
    "            \n",
    "            results.append({\n",
    "                'request_id': req_id,\n",
    "                'user_id': req_info['user_id'],\n",
    "                'status_code': resp_info['status_code'],\n",
    "                'duration_ms': int(duration)\n",
    "            })\n",
    "\n",
    "    # 4. Write all results at once\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "# Example Usage (assuming you create these files):\n",
    "junior_correlator('requests.log', 'responses.log', 'output_junior.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d83915-cd24-418f-b358-826db9a9523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Iterator\n",
    "\n",
    "def parse_log_line(line: str) -> Dict[str, Any]:\n",
    "    \"\"\"Parses a single log line and handles potential errors.\"\"\"\n",
    "    try:\n",
    "        parts = line.strip().split(',')\n",
    "        timestamp = datetime.fromisoformat(parts[0].replace('Z', '+00:00'))\n",
    "        return {'timestamp': timestamp, 'parts': parts[1:]}\n",
    "    except (ValueError, IndexError):\n",
    "        # Log this error in a real system\n",
    "        return None\n",
    "\n",
    "def mid_level_correlator(requests_file: str, responses_file: str) -> Iterator[Dict[str, Any]]:\n",
    "    \"\"\"Correlates logs and yields results one by one.\"\"\"\n",
    "    pending_requests = {}\n",
    "    \n",
    "    # Pass 1: Process all requests first\n",
    "    with open(requests_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parsed = parse_log_line(line)\n",
    "            if parsed:\n",
    "                req_id, user_id = parsed['parts']\n",
    "                pending_requests[req_id] = {\n",
    "                    'start_time': parsed['timestamp'],\n",
    "                    'user_id': user_id\n",
    "                }\n",
    "\n",
    "    # Pass 2: Process responses and find matches\n",
    "    with open(responses_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parsed = parse_log_line(line)\n",
    "            if not parsed:\n",
    "                continue\n",
    "            \n",
    "            resp_id, status_code = parsed['parts']\n",
    "            if resp_id in pending_requests:\n",
    "                req_info = pending_requests[resp_id]\n",
    "                duration = (parsed['timestamp'] - req_info['start_time']).total_seconds() * 1000\n",
    "                \n",
    "                yield {\n",
    "                    'request_id': resp_id,\n",
    "                    'user_id': req_info['user_id'],\n",
    "                    'status_code': int(status_code),\n",
    "                    'duration_ms': int(duration)\n",
    "                }\n",
    "                # Remove to handle potential duplicate request_ids\n",
    "                del pending_requests[resp_id]\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the correlation and write to a file.\"\"\"\n",
    "    with open('output_mid.json', 'w') as f:\n",
    "        # The generator approach prevents building a huge list in memory\n",
    "        results = list(mid_level_correlator('requests.log', 'responses.log'))\n",
    "        json.dump(results, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50fdd523-9dad-463b-9c3a-5e0262bf1d31",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7413475-875f-4ce0-80f9-769fb890a7bf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/Users/jesses_fables/Desktop/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/jesses_fables/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /Users/jesses_fables/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e6984e08-91a7-429c-b435-b24be0c1202b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 280ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e6984e08-91a7-429c-b435-b24be0c1202b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/9ms)\n",
      "25/09/07 12:57:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/07 12:57:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/09/07 12:57:45 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/09/07 12:57:45 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/09/07 12:57:45 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark and Delta Lake are ready.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"LogLakeZero\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "# Create the SparkSession\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print(\"Spark and Delta Lake are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3cb2b93-49fc-4daa-bbd4-f8c165c49ddf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "requests_df = pd.read_csv('requests.log', names = ['TIMESTAMP_ISO8601','REQUEST_ID','USER_ID'])\n",
    "responses_df = pd.read_csv('responses.log', names = ['TIMESTAMP_ISO8601','REQUEST_ID','STATUS_CODE'])\n",
    "\n",
    "#Create PySpark DataFrame from Pandas\n",
    "sparkDF_requests_df=spark.createDataFrame(requests_df) \n",
    "##sparkDF_requests_df.printSchema()\n",
    "##sparkDF_requests_df.show()\n",
    "\n",
    "sparkDF_responses_df=spark.createDataFrame(responses_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fde7d80-c9ef-454b-bd36-34153e6073ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/07 13:10:10 WARN TaskSetManager: Stage 5 contains a task of very large size (2110 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/07 13:10:17 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "sparkDF_requests_df.write.format(\"delta\").mode(\"overwrite\").save('files/requests_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c578af95-3b46-4b8a-8590-cf5cac1ec291",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/07 13:10:44 WARN TaskSetManager: Stage 9 contains a task of very large size (1941 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "sparkDF_responses_df.write.format(\"delta\").mode(\"overwrite\").save('files/responses_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3861c3b0-9a19-43f9-a41d-af2e43d64eee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/07 13:20:24 WARN TaskSetManager: Stage 46 contains a task of very large size (2110 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(REQUEST_ID='req-f0588781-cfd6-4abe-b2a2-adf65699a24e', count=1),\n",
       " Row(REQUEST_ID='req-86f954e5-cf6e-4921-8434-849c90f4ddc8', count=1),\n",
       " Row(REQUEST_ID='req-4b69dc47-c2fe-47b5-af20-ddc21de6c766', count=1),\n",
       " Row(REQUEST_ID='req-orphan-abd8ea83-4180-4715-b3f8-10b107da6618', count=1),\n",
       " Row(REQUEST_ID='req-3305f84d-a721-4f0e-abdb-fe6595c3fde0', count=1)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDF_requests_df.groupBy(\"REQUEST_ID\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False) \\\n",
    "    .head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e0f28574-e95d-4a01-915c-9a5752141d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/07 13:36:39 WARN TaskSetManager: Stage 95 contains a task of very large size (1941 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total oprhans (using collect): 5,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "sparkDF_responses_df.createOrReplaceTempView(\"responses\")\n",
    "sparkDF_requests_df.createOrReplaceTempView(\"requests\")\n",
    "\n",
    "orphan_requests = spark.sql(\"SELECT count(*) as result from responses where REQUEST_ID LIKE '%orphan%'\")\n",
    "orphan_requests_count=orphan_requests.collect()[0][0]\n",
    "print(f\"Total oprhans (using collect): {orphan_requests_count:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14382eee-827b-49b7-bc6a-f64eb6abc3c5",
   "metadata": {},
   "source": [
    "You are a backend engineer for a large web service. The service generates two distinct log files:\n",
    "\n",
    "1.  **`requests.log`**: Records when a request is received.\n",
    "      * Format: `TIMESTAMP_ISO8601,REQUEST_ID,USER_ID`\n",
    "      * Example: `2025-09-07T12:15:01.123Z,req-abc,user-123`\n",
    "2.  **`responses.log`**: Records when a response is sent.\n",
    "      * Format: `TIMESTAMP_ISO8601,REQUEST_ID,STATUS_CODE`\n",
    "      * Example: `2025-09-07T12:15:01.345Z,req-abc,200`\n",
    "\n",
    "Your task is to write a Python script that **correlates** these two logs and produces a single, combined JSON output. For each request, the output should include the `request_id`, `user_id`, `status_code`, and a calculated `duration_ms`.\n",
    "\n",
    "#### The Challenges (What makes it \"hard\")\n",
    "\n",
    "  * **Scalability**: The log files are **too large to fit into memory**.\n",
    "  * **Unordered Entries**: The logs are not guaranteed to be in chronological order. A response might be logged before its corresponding request.\n",
    "  * **Orphaned Entries**: A `request_id` might appear in one file but not the other due to network errors or crashes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f2f7ba-4f19-402b-bc20-a88c76bce785",
   "metadata": {},
   "source": [
    "You are a backend engineer for a large web service. The service generates two distinct log files:\n",
    "\n",
    "1.  **`requests.log`**: Records when a request is received.\n",
    "      * Format: `TIMESTAMP_ISO8601,REQUEST_ID,USER_ID`\n",
    "      * Example: `2025-09-07T12:15:01.123Z,req-abc,user-123`\n",
    "2.  **`responses.log`**: Records when a response is sent.\n",
    "      * Format: `TIMESTAMP_ISO8601,REQUEST_ID,STATUS_CODE`\n",
    "      * Example: `2025-09-07T12:15:01.345Z,req-abc,200`\n",
    "\n",
    "Your task is to write a Python script that **correlates** these two logs and produces a single, combined JSON output. For each request, the output should include the `request_id`, `user_id`, `status_code`, and a calculated `duration_ms`.\n",
    "\n",
    "#### The Challenges (What makes it \"hard\")\n",
    "\n",
    "  * **Scalability**: The log files are **too large to fit into memory**.\n",
    "  * **Unordered Entries**: The logs are not guaranteed to be in chronological order. A response might be logged before its corresponding request.\n",
    "  * **Orphaned Entries**: A `request_id` might appear in one file but not the other due to network errors or crashes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7830947a-458a-4fd8-8a5f-8de7b7832b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_table = spark.sql(\"\"\"\n",
    "SELECT a.TIMESTAMP_ISO8601 as start_time, b.TIMESTAMP_ISO8601 as end_time, a.REQUEST_ID, a.USER_ID, b.STATUS_CODE\n",
    "FROM requests as a\n",
    "LEFT JOIN responses as b on a.REQUEST_ID = b.REQUEST_ID\n",
    "\"\"\")\n",
    "joined_table.createOrReplaceTempView(\"joined_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6ea3586c-2d09-48c1-b214-54f4beffaf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = spark.sql('SELECT * FROM joined_view UNION SELECT * FROM joined_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e9fa257c-c756-4013-a7ab-ad504b4fcb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm.createOrReplaceTempView(\"hmm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7086f796-8d33-461e-a01e-1ad8d960dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm3 = spark.sql(\"SELECT * FROM hmm where request_id like '%orphan%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0c6a4146-1537-43bc-a784-e9d426b9b30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/07 13:48:05 WARN TaskSetManager: Stage 125 contains a task of very large size (2110 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/07 13:48:06 WARN TaskSetManager: Stage 126 contains a task of very large size (1941 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "35a8d3d3-b8e2-4d6b-aca2-34fd97ced3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/07 13:49:26 WARN TaskSetManager: Stage 152 contains a task of very large size (2110 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/07 13:49:26 WARN TaskSetManager: Stage 153 contains a task of very large size (1941 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "210000"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7e85a612-8f3a-41c7-b6b5-d9186e808641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/07 13:49:02 WARN TaskSetManager: Stage 143 contains a task of very large size (1941 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/07 13:49:02 WARN TaskSetManager: Stage 144 contains a task of very large size (2110 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "105000"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4c56a88a-d26f-412a-9781-ab2278258324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/07 14:04:30 WARN TaskSetManager: Stage 253 contains a task of very large size (2110 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/07 14:04:30 WARN TaskSetManager: Stage 254 contains a task of very large size (1941 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 254:>                                                        (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----------+-----------+\n",
      "|          request_id| user_id|status_code|duration_ms|\n",
      "+--------------------+--------+-----------+-----------+\n",
      "|req-000272c0-2d42...| user-94|        200|       1357|\n",
      "|req-00030e39-a5a5...| user-59|        200|       1857|\n",
      "|req-0006f648-8d39...|user-209|        200|        629|\n",
      "|req-0007b1d0-3373...| user-68|        503|       1977|\n",
      "|req-0009006d-36ff...| user-73|        200|        296|\n",
      "|req-000a83e4-c15a...|user-261|        200|       1517|\n",
      "|req-000b3223-af78...| user-45|        200|        741|\n",
      "|req-000d9994-48d8...|user-485|        404|       1323|\n",
      "|req-000efa6e-45f2...|user-641|        404|        144|\n",
      "|req-0012220b-15e4...|user-975|        200|       1301|\n",
      "|req-0012f62c-9711...|user-110|        200|        367|\n",
      "|req-00149896-4ada...|user-804|        200|        712|\n",
      "|req-00159c3e-f544...| user-39|        200|        730|\n",
      "|req-0016ff15-7075...|user-244|        200|        822|\n",
      "|req-0017095d-e64f...| user-77|        200|       1617|\n",
      "|req-00199e99-1c2b...| user-20|        200|        312|\n",
      "|req-001a49ad-4550...|user-244|        200|        609|\n",
      "|req-001a87d8-dec6...| user-31|        404|        269|\n",
      "|req-001afb7b-8aea...|user-895|        200|       1663|\n",
      "|req-00219505-2f16...|user-732|        404|        820|\n",
      "+--------------------+--------+-----------+-----------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "result_log = spark.sql(\"\"\"\n",
    "SELECT\n",
    "request_id,\n",
    "user_id,\n",
    "status_code,\n",
    "TIMESTAMPDIFF(MILLISECOND, start_time, end_time) as duration_ms\n",
    "from joined_view\n",
    "WHERE status_code IS NOT NULL\n",
    "\"\"\")\n",
    "result_log.createOrReplaceTempView(\"result_log_success\")\n",
    "result_log.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
