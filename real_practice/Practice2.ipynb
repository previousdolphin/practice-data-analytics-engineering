{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6210889f-6101-4019-8efe-14a46a394c3a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### The Problem: Log File Correlator\n",
    "\n",
    "You are a backend engineer for a large web service. The service generates two distinct log files:\n",
    "\n",
    "1.  **`requests.log`**: Records when a request is received.\n",
    "      * Format: `TIMESTAMP_ISO8601,REQUEST_ID,USER_ID`\n",
    "      * Example: `2025-09-07T12:15:01.123Z,req-abc,user-123`\n",
    "2.  **`responses.log`**: Records when a response is sent.\n",
    "      * Format: `TIMESTAMP_ISO8601,REQUEST_ID,STATUS_CODE`\n",
    "      * Example: `2025-09-07T12:15:01.345Z,req-abc,200`\n",
    "\n",
    "Your task is to write a Python script that **correlates** these two logs and produces a single, combined JSON output. For each request, the output should include the `request_id`, `user_id`, `status_code`, and a calculated `duration_ms`.\n",
    "\n",
    "#### The Challenges (What makes it \"hard\")\n",
    "\n",
    "  * **Scalability**: The log files are **too large to fit into memory**.\n",
    "  * **Unordered Entries**: The logs are not guaranteed to be in chronological order. A response might be logged before its corresponding request.\n",
    "  * **Orphaned Entries**: A `request_id` might appear in one file but not the other due to network errors or crashes.\n",
    "\n",
    "-----\n",
    "\n",
    "### Level 1: Junior Engineer Solution (\"It Works\")\n",
    "\n",
    "This solution correctly solves the problem for small files but ignores the scalability and robustness constraints. It's a good starting point that demonstrates basic Python skills.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "  * Reads entire files into memory.\n",
    "  * Uses multiple loops and basic dictionaries.\n",
    "  * Minimal error handling.\n",
    "  * Contained within a single script or function.\n",
    "\n",
    "<!-- end list -->\n",
    "\n",
    "```python\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def junior_correlator(requests_file, responses_file, output_file):\n",
    "    requests_data = {}\n",
    "    # 1. Read all requests into a dictionary\n",
    "    with open(requests_file, 'r') as f:\n",
    "        for line in f:\n",
    "            timestamp_str, request_id, user_id = line.strip().split(',')\n",
    "            requests_data[request_id] = {\n",
    "                'start_time': datetime.fromisoformat(timestamp_str.replace('Z', '+00:00')),\n",
    "                'user_id': user_id\n",
    "            }\n",
    "\n",
    "    responses_data = {}\n",
    "    # 2. Read all responses into another dictionary\n",
    "    with open(responses_file, 'r') as f:\n",
    "        for line in f:\n",
    "            timestamp_str, request_id, status_code = line.strip().split(',')\n",
    "            responses_data[request_id] = {\n",
    "                'end_time': datetime.fromisoformat(timestamp_str.replace('Z', '+00:00')),\n",
    "                'status_code': int(status_code)\n",
    "            }\n",
    "\n",
    "    results = []\n",
    "    # 3. Loop through requests to find matching responses\n",
    "    for req_id, req_info in requests_data.items():\n",
    "        if req_id in responses_data:\n",
    "            resp_info = responses_data[req_id]\n",
    "            duration = (resp_info['end_time'] - req_info['start_time']).total_seconds() * 1000\n",
    "            \n",
    "            results.append({\n",
    "                'request_id': req_id,\n",
    "                'user_id': req_info['user_id'],\n",
    "                'status_code': resp_info['status_code'],\n",
    "                'duration_ms': int(duration)\n",
    "            })\n",
    "\n",
    "    # 4. Write all results at once\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "# Example Usage (assuming you create these files):\n",
    "# junior_correlator('requests.log', 'responses.log', 'output_junior.json')\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "### Level 2: Mid-Level Engineer Solution (\"It's Well-Crafted\")\n",
    "\n",
    "This solution addresses some of the junior version's shortcomings. It uses better data structures and practices, showing an understanding of code organization and efficiency, though it still has memory limitations.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "  * Uses a single pass over the second file to enrich data from the first.\n",
    "  * Handles potential errors during parsing (e.g., a malformed line).\n",
    "  * Code is broken down into logical functions.\n",
    "  * Uses type hints and generators for better memory management of the *output*.\n",
    "\n",
    "<!-- end list -->\n",
    "\n",
    "```python\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Iterator\n",
    "\n",
    "def parse_log_line(line: str) -> Dict[str, Any]:\n",
    "    \"\"\"Parses a single log line and handles potential errors.\"\"\"\n",
    "    try:\n",
    "        parts = line.strip().split(',')\n",
    "        timestamp = datetime.fromisoformat(parts[0].replace('Z', '+00:00'))\n",
    "        return {'timestamp': timestamp, 'parts': parts[1:]}\n",
    "    except (ValueError, IndexError):\n",
    "        # Log this error in a real system\n",
    "        return None\n",
    "\n",
    "def mid_level_correlator(requests_file: str, responses_file: str) -> Iterator[Dict[str, Any]]:\n",
    "    \"\"\"Correlates logs and yields results one by one.\"\"\"\n",
    "    pending_requests = {}\n",
    "    \n",
    "    # Pass 1: Process all requests first\n",
    "    with open(requests_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parsed = parse_log_line(line)\n",
    "            if parsed:\n",
    "                req_id, user_id = parsed['parts']\n",
    "                pending_requests[req_id] = {\n",
    "                    'start_time': parsed['timestamp'],\n",
    "                    'user_id': user_id\n",
    "                }\n",
    "\n",
    "    # Pass 2: Process responses and find matches\n",
    "    with open(responses_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parsed = parse_log_line(line)\n",
    "            if not parsed:\n",
    "                continue\n",
    "            \n",
    "            resp_id, status_code = parsed['parts']\n",
    "            if resp_id in pending_requests:\n",
    "                req_info = pending_requests[resp_id]\n",
    "                duration = (parsed['timestamp'] - req_info['start_time']).total_seconds() * 1000\n",
    "                \n",
    "                yield {\n",
    "                    'request_id': resp_id,\n",
    "                    'user_id': req_info['user_id'],\n",
    "                    'status_code': int(status_code),\n",
    "                    'duration_ms': int(duration)\n",
    "                }\n",
    "                # Remove to handle potential duplicate request_ids\n",
    "                del pending_requests[resp_id]\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the correlation and write to a file.\"\"\"\n",
    "    with open('output_mid.json', 'w') as f:\n",
    "        # The generator approach prevents building a huge list in memory\n",
    "        results = list(mid_level_correlator('requests.log', 'responses.log'))\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "### Level 3: Senior Engineer Solution (\"It's Production-Ready\")\n",
    "\n",
    "This solution is designed for scalability and robustness. It correctly handles the \"too large for memory\" constraint by processing the files in a streaming fashion.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "  * **Streaming approach**: Never holds the full dataset in memory. Memory is only used for *in-flight* requests.\n",
    "  * **Object-Oriented Design**: Encapsulates logic in a class, making it maintainable and testable.\n",
    "  * **Robust Error Handling**: Uses Python's `logging` module to report issues like orphaned entries.\n",
    "  * **Configurability**: Uses `argparse` to accept file paths from the command line, making it a flexible tool.\n",
    "\n",
    "<!-- end list -->\n",
    "\n",
    "```python\n",
    "import json\n",
    "import logging\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Iterator\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class LogCorrelator:\n",
    "    \"\"\"\n",
    "    Correlates request and response logs in a memory-efficient, streaming manner.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.pending_requests: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    def _process_request_line(self, line: str):\n",
    "        try:\n",
    "            timestamp_str, req_id, user_id = line.strip().split(',')\n",
    "            # If a response arrived first, complete the record\n",
    "            if req_id in self.pending_requests and 'end_time' in self.pending_requests[req_id]:\n",
    "                self.pending_requests[req_id].update({\n",
    "                    'start_time': datetime.fromisoformat(timestamp_str.replace('Z', '+00:00')),\n",
    "                    'user_id': user_id\n",
    "                })\n",
    "            else: # Otherwise, store the request data\n",
    "                self.pending_requests[req_id] = {\n",
    "                    'start_time': datetime.fromisoformat(timestamp_str.replace('Z', '+00:00')),\n",
    "                    'user_id': user_id\n",
    "                }\n",
    "        except (ValueError, IndexError):\n",
    "            logging.warning(f\"Skipping malformed request line: {line.strip()}\")\n",
    "\n",
    "    def _process_response_line(self, line: str) -> Iterator[Dict[str, Any]]:\n",
    "        try:\n",
    "            timestamp_str, resp_id, status_code = line.strip().split(',')\n",
    "            end_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))\n",
    "\n",
    "            # If the request arrived first, complete and yield the record\n",
    "            if resp_id in self.pending_requests and 'start_time' in self.pending_requests[resp_id]:\n",
    "                req_info = self.pending_requests.pop(resp_id) # Pop to free memory\n",
    "                duration = (end_time - req_info['start_time']).total_seconds() * 1000\n",
    "                yield {\n",
    "                    'request_id': resp_id,\n",
    "                    'user_id': req_info['user_id'],\n",
    "                    'status_code': int(status_code),\n",
    "                    'duration_ms': int(duration)\n",
    "                }\n",
    "            else: # Otherwise, store the response as pending\n",
    "                self.pending_requests[resp_id] = {\n",
    "                    'end_time': end_time,\n",
    "                    'status_code': int(status_code)\n",
    "                }\n",
    "        except (ValueError, IndexError):\n",
    "            logging.warning(f\"Skipping malformed response line: {line.strip()}\")\n",
    "\n",
    "    def correlate(self, requests_file: str, responses_file: str) -> Iterator[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Processes both log files and yields correlated results.\n",
    "        This approach assumes files are of similar size and processes them \"together\".\n",
    "        A more advanced solution for disparate file sizes might process one fully first.\n",
    "        \"\"\"\n",
    "        logging.info(\"Starting log correlation.\")\n",
    "        with open(requests_file, 'r') as req_f, open(responses_file, 'r') as resp_f:\n",
    "            # In a real-world scenario with massive files, you might read them in chunks\n",
    "            # or use a more sophisticated streaming library.\n",
    "            for req_line, resp_line in zip(req_f, resp_f):\n",
    "                self._process_request_line(req_line)\n",
    "                yield from self._process_response_line(resp_line)\n",
    "\n",
    "        # After files are processed, log any remaining orphaned entries\n",
    "        orphaned_count = len(self.pending_requests)\n",
    "        if orphaned_count > 0:\n",
    "            logging.warning(f\"Found {orphaned_count} orphaned log entries.\")\n",
    "        logging.info(\"Correlation complete.\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Correlate web service log files.\")\n",
    "    parser.add_argument(\"requests_file\", help=\"Path to the requests log file.\")\n",
    "    parser.add_argument(\"responses_file\", help=\"Path to the responses log file.\")\n",
    "    parser.add_argument(\"output_file\", help=\"Path for the JSON output file.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    correlator = LogCorrelator()\n",
    "    with open(args.output_file, 'w') as f:\n",
    "        results = list(correlator.correlate(args.requests_file, args.responses_file))\n",
    "        json.dump(results, f, indent=2)\n",
    "    logging.info(f\"Successfully wrote {len(results)} records to {args.output_file}\")\n",
    "\n",
    "# To run from the command line:\n",
    "# python your_script_name.py requests.log responses.log output_senior.json\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15649493-b186-424a-8a34-27f5caec25bf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def junior_correlator(requests_file, responses_file, output_file):\n",
    "    requests_data = {}\n",
    "    # 1. Read all requests into a dictionary\n",
    "    with open(requests_file, 'r') as f:\n",
    "        for line in f:\n",
    "            timestamp_str, request_id, user_id = line.strip().split(',')\n",
    "            requests_data[request_id] = {\n",
    "                'start_time': datetime.fromisoformat(timestamp_str.replace('Z', '+00:00')),\n",
    "                'user_id': user_id\n",
    "            }\n",
    "\n",
    "    responses_data = {}\n",
    "    # 2. Read all responses into another dictionary\n",
    "    with open(responses_file, 'r') as f:\n",
    "        for line in f:\n",
    "            timestamp_str, request_id, status_code = line.strip().split(',')\n",
    "            responses_data[request_id] = {\n",
    "                'end_time': datetime.fromisoformat(timestamp_str.replace('Z', '+00:00')),\n",
    "                'status_code': int(status_code)\n",
    "            }\n",
    "\n",
    "    results = []\n",
    "    # 3. Loop through requests to find matching responses\n",
    "    for req_id, req_info in requests_data.items():\n",
    "        if req_id in responses_data:\n",
    "            resp_info = responses_data[req_id]\n",
    "            duration = (resp_info['end_time'] - req_info['start_time']).total_seconds() * 1000\n",
    "            \n",
    "            results.append({\n",
    "                'request_id': req_id,\n",
    "                'user_id': req_info['user_id'],\n",
    "                'status_code': resp_info['status_code'],\n",
    "                'duration_ms': int(duration)\n",
    "            })\n",
    "\n",
    "    # 4. Write all results at once\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "# Example Usage (assuming you create these files):\n",
    "junior_correlator('requests.log', 'responses.log', 'output_junior.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d83915-cd24-418f-b358-826db9a9523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Iterator\n",
    "\n",
    "def parse_log_line(line: str) -> Dict[str, Any]:\n",
    "    \"\"\"Parses a single log line and handles potential errors.\"\"\"\n",
    "    try:\n",
    "        parts = line.strip().split(',')\n",
    "        timestamp = datetime.fromisoformat(parts[0].replace('Z', '+00:00'))\n",
    "        return {'timestamp': timestamp, 'parts': parts[1:]}\n",
    "    except (ValueError, IndexError):\n",
    "        # Log this error in a real system\n",
    "        return None\n",
    "\n",
    "def mid_level_correlator(requests_file: str, responses_file: str) -> Iterator[Dict[str, Any]]:\n",
    "    \"\"\"Correlates logs and yields results one by one.\"\"\"\n",
    "    pending_requests = {}\n",
    "    \n",
    "    # Pass 1: Process all requests first\n",
    "    with open(requests_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parsed = parse_log_line(line)\n",
    "            if parsed:\n",
    "                req_id, user_id = parsed['parts']\n",
    "                pending_requests[req_id] = {\n",
    "                    'start_time': parsed['timestamp'],\n",
    "                    'user_id': user_id\n",
    "                }\n",
    "\n",
    "    # Pass 2: Process responses and find matches\n",
    "    with open(responses_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parsed = parse_log_line(line)\n",
    "            if not parsed:\n",
    "                continue\n",
    "            \n",
    "            resp_id, status_code = parsed['parts']\n",
    "            if resp_id in pending_requests:\n",
    "                req_info = pending_requests[resp_id]\n",
    "                duration = (parsed['timestamp'] - req_info['start_time']).total_seconds() * 1000\n",
    "                \n",
    "                yield {\n",
    "                    'request_id': resp_id,\n",
    "                    'user_id': req_info['user_id'],\n",
    "                    'status_code': int(status_code),\n",
    "                    'duration_ms': int(duration)\n",
    "                }\n",
    "                # Remove to handle potential duplicate request_ids\n",
    "                del pending_requests[resp_id]\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the correlation and write to a file.\"\"\"\n",
    "    with open('output_mid.json', 'w') as f:\n",
    "        # The generator approach prevents building a huge list in memory\n",
    "        results = list(mid_level_correlator('requests.log', 'responses.log'))\n",
    "        json.dump(results, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50fdd523-9dad-463b-9c3a-5e0262bf1d31",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7413475-875f-4ce0-80f9-769fb890a7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] requests_file responses_file output_file\n",
      "ipykernel_launcher.py: error: the following arguments are required: responses_file, output_file\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
