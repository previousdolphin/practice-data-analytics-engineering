{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8fc05bf-c471-4182-aa20-f5dc46105c91",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##Reddit Practice Problem: Surging Subreddits##\n",
    "\n",
    "The Scenario\n",
    "You're an Analytics Engineering Lead at Reddit. The \"Discovery\" team wants to identify subreddits that are experiencing a sudden surge in user engagement. They hypothesize that a rapid increase in the number of comments per post is a key indicator of a subreddit \"going viral\" or becoming a hot topic.\n",
    "\n",
    "Your task is to build a data pipeline that identifies the top 10 subreddits with the highest week-over-week percentage growth in their average comments per post.\n",
    "\n",
    "Input DataFrames\n",
    "You are given two primary sources of data:\n",
    "\n",
    "posts DataFrame: Contains information about each post submitted.\n",
    "\n",
    "post_id (string)\n",
    "\n",
    "subreddit_id (string)\n",
    "\n",
    "created_utc (timestamp)\n",
    "\n",
    "comments DataFrame: A log of all comments made on posts.\n",
    "\n",
    "comment_id (string)\n",
    "\n",
    "post_id (string)\n",
    "\n",
    "created_utc (timestamp)\n",
    "\n",
    "The Task\n",
    "Write a PySpark script that performs the following steps:\n",
    "\n",
    "Calculate Weekly Comment Counts: For each post, determine how many comments it received. Then, aggregate this data to find the total number of comments and the total number of posts for each subreddit, for each week.\n",
    "\n",
    "A \"week\" can be defined using the weekofyear function.\n",
    "\n",
    "Calculate Average Comments Per Post: Using the weekly aggregated data from Step 1, calculate the average number of comments per post for each subreddit for each week.\n",
    "\n",
    "Find Previous Week's Average: For each subreddit and each week, you need to find the average comments per post from the previous week. This is the key step and will require a window function partitioned by subreddit and ordered by week.\n",
    "\n",
    "Calculate Week-over-Week Growth: Calculate the percentage growth from the previous week's average to the current week's average. The formula is: ((current_avg - previous_avg) / previous_avg) * 100.\n",
    "\n",
    "Handle cases where the previous week's average was zero to avoid division errors.\n",
    "\n",
    "Filter and Rank: Filter for the most recent complete week in the dataset. Then, rank the subreddits by their week-over-week growth in descending order and return the top 10.\n",
    "\n",
    "Expected Final Output\n",
    "A DataFrame with the following schema, showing the top 10 surging subreddits for the most recent week:\n",
    "\n",
    "subreddit_id\n",
    "\n",
    "week\n",
    "\n",
    "avg_comments_per_post\n",
    "\n",
    "previous_week_avg_comments\n",
    "\n",
    "wow_growth_percentage\n",
    "\n",
    "rank\n",
    "\n",
    "Follow-up Questions for a Lead Role\n",
    "Optimization: This job could be slow if the comments table is huge. How would you optimize the initial join between posts and comments? What if there's data skew in a few very popular posts? (Probes knowledge of partitioning, broadcasting, and salting).\n",
    "\n",
    "Data Modeling: How would you productionize this logic? Would you create an intermediate, aggregated table? What would be the \"grain\" of that table (daily, weekly)? How would you handle late-arriving data?\n",
    "\n",
    "Edge Cases: What are the flaws in this \"surge\" logic? What if a subreddit is new and has no \"previous week\" data? How should its growth be represented? What if a subreddit's activity is so low that going from 1 comment to 2 registers as 100% growth? How might you refine the logic to account for this?\n",
    "\n",
    "Definition of \"Week\": We used weekofyear. What are the potential issues with this function, especially around the new year? (e.g., Week 52 vs. Week 1). What is a more robust way to define a \"week\"? (Probes deeper SQL/Spark function knowledge, like using date_trunc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ef628c-5675-491e-935e-f5c3c44b3f84",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/07 12:13:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/07 12:13:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/09/07 12:13:38 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, count, weekofyear, lag, when, lit, desc, rank\n",
    "\n",
    "# --- 1. Setup Spark Session and Create Dummy Data ---\n",
    "spark = SparkSession.builder.appName(\"RedditSurgeAnalysis\").getOrCreate()\n",
    "\n",
    "posts_data = [(\"p1\", \"sub1\", \"2025-08-20 10:00:00\"), (\"p2\", \"sub1\", \"2025-08-28 11:00:00\"),\n",
    "              (\"p3\", \"sub2\", \"2025-08-21 12:00:00\"), (\"p4\", \"sub2\", \"2025-08-29 13:00:00\"),\n",
    "              (\"p5\", \"sub3\", \"2025-08-22 14:00:00\"), (\"p6\", \"sub3\", \"2025-08-30 15:00:00\")]\n",
    "posts = spark.createDataFrame(posts_data, [\"post_id\", \"subreddit_id\", \"created_utc\"]) \\\n",
    "    .withColumn(\"created_utc\", col(\"created_utc\").cast(\"timestamp\"))\n",
    "\n",
    "comments_data = [(\"c1\", \"p1\", \"2025-08-30 15:01:00\"), (\"c2\", \"p1\", \"2025-08-30 15:02:00\"), (\"c3\", \"p2\", \"2025-08-30 15:03:00\"), # sub1\n",
    "                 (\"c4\", \"p3\", \"2025-08-30 15:04:00\"), (\"c5\", \"p4\", \"2025-08-30 15:05:00\"), (\"c6\", \"p4\", \"2025-08-30 15:06:00\"), (\"c7\", \"p4\", \"2025-08-30 15:07:00\"), # sub2\n",
    "                 (\"c8\", \"p5\", \"2025-08-30 15:08:00\"), (\"c9\", \"p6\", \"2025-08-30 15:09:00\"), (\"c10\", \"p6\", \"2025-08-30 15:11:00\")] # sub3\n",
    "comments = spark.createDataFrame(comments_data, [\"comment_id\", \"post_id\", \"created_utc\"]) \\\n",
    "    .withColumn(\"created_utc\", col(\"created_utc\").cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446db63-8023-4ca1-806b-2992243a631c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
